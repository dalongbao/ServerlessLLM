# ServerlessLLM Docker Compose Configuration
#
# This configuration includes:
# 1. MinIO for local file/dataset storage (compatible with S3 API)
# 2. Head node (API server + controller)
# 3. Worker node(s) with GPU access
#
# File Storage Options:
# - Local: Uses MinIO container (default configuration below)
# - Cloud: Change S3_* environment variables to point to AWS S3, Cloudflare R2, etc.
#
# Usage:
#   docker-compose up -d
#   Access MinIO console: http://localhost:9001 (admin/password123)
#   Access ServerlessLLM API: http://localhost:8343

services:
  # MinIO Storage Service (for file/dataset storage)
  minio:
    image: minio/minio:latest
    container_name: sllm_minio
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password123
    ports:
      - "9000:9000"    # MinIO API
      - "9001:9001"    # MinIO Console
    networks:
      - sllm_network
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"

  # Head Node
  sllm_head:
    build:
      context: ../../
      dockerfile: Dockerfile
    image: serverlessllm/sllm:latest
    container_name: sllm_head
    environment:
      - MODEL_FOLDER=${MODEL_FOLDER}
      - MODE=HEAD
      # File Storage Configuration (MinIO/S3)
      - S3_ENDPOINT_URL=http://minio:9000     # Local MinIO (change for cloud S3/R2)
      - S3_ACCESS_KEY=admin                   # MinIO/S3 access key
      - S3_SECRET_KEY=password123             # MinIO/S3 secret key
      - S3_BUCKET=sllm-datasets               # Bucket name for datasets/files
      - S3_REGION=us-east-1                   # S3 region
    ports:
      - "6379:6379"    # Redis port
      - "8343:8343"    # ServerlessLLM port
    networks:
      - sllm_network
    depends_on:
      - minio
    command: []

  # Worker Node 0
  sllm_worker_0:
    build:
      context: ../../
      dockerfile: Dockerfile
    image: serverlessllm/sllm:latest
    container_name: sllm_worker_0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"] # Assigns GPU 0 to the worker
    environment:
      - WORKER_ID=0
      - STORAGE_PATH=/models
      - MODE=WORKER
    networks:
      - sllm_network
    volumes:
      - ${MODEL_FOLDER}:/models
    command: ["--mem-pool-size", "4GB", "--registration-required", "true"] # Customize the memory pool size here

networks:
  sllm_network:
    driver: bridge
    name: sllm

volumes:
  minio_data:
    driver: local
